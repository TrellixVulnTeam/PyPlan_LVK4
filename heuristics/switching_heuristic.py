from abstract import absheuristic


class SwitchingHeuristicClass(absheuristic.AbstractHeuristic):
    """Given a set of policies, calculate the rewards generated by the best policy at each state."""
    myname = "Switching Heuristic"

    def __init__(self, switch_policies, width=1, depth=10):
        self.heuristicname = self.myname
        self.switch_policies = switch_policies
        self.width = width
        self.depth = depth

    def get_heuristic_name(self):
        return self.agentname

    def evaluate(self, state):
        """Evaluate the state using the heuristic parameters (width and depth) and best of the rollout policies."""
        sim_state = state.clone()  # create the simulated state so that the current state is left unchanged
        total_rewards = [[0]*sim_state.number_of_players()] * len(self.switch_policies)

        for idx, policy in enumerate(self.switch_policies):  # run each policy width times up to depth
            for sim_num in range(self.width):  # run width simulations
                h = 0  # reset depth counter
                while sim_state.is_terminal() is False and h <= self.depth:  # act and track rewards as long as possible
                    action_to_take = policy.select_action(sim_state)
                    rewards = sim_state.take_action(action_to_take)
                    total_rewards[idx] = [sum(r) for r in zip(total_rewards[idx], rewards)]
                    h += 1
                sim_state.set(state)  # reset state

        best_reward = float("-inf")  # rewards of best policy
        for idx, rewards in enumerate(total_rewards):
            if rewards[0] > best_reward:  # reward for our agent
                best_reward = rewards[0]
                best_idx = idx

        return [r / self.width for r in total_rewards[best_idx]]  # average rewards over each of width simulations
